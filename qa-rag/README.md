# Simple Retrieval Augmented Generation (RAG) from Scratch

This small project shows how to build a basic Question-Answering chatbot based on the [Retrieval-Augmented Generation (RAG)](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) pattern from scratch.

RAG consists in 

- retrieving relevant documents related to the user **query** 
- and feeding them to a generative model in the context along with the query, while we instruct it to provide the answer using the provided documents/context.

Therefore, we avoid needing to fine-tune the generative model with our documents.
This is specially well suited when we want to extend the model's *memory* with recent and continuously changing documents, aka. the *knowledge base*.

In order to show how the approach works,

- I use the **large language model (LLM)** [`gpt-3.5-turbo-instruct`](https://platform.openai.com/docs/models/gpt-3.5-turbo?snapshot=gpt-3.5-turbo-instruct) from OpenAI
- and a **dataset** or set of **queried documents** built from the Wikipedia article [2024 Events in Spain](https://en.wikipedia.org/wiki/2024_in_Spain) (54 events in total).

The [`gpt-3.5-turbo-instruct`](https://platform.openai.com/docs/models/gpt-3.5-turbo?snapshot=gpt-3.5-turbo-instruct) model

- is a Legacy GPT model for cheaper chat and non-chat tasks,
- has a context window of `4,096` tokens,
- and has a **knowledge cutoff as of Sep 01, 2021**.

Therefore, we can be sure that none of the 2024 events in Spain were used for during the model training. Therefore,

- if we ask the model a question about the dataset, it should hallucinate and/or fail to answer properly;
- but if we use the RAG pattern, it should be able to build and use a relevant context that facilitates a correct answer.

One can say that RAG systems have two major pipelines, and each of them gets a section here:

- [Dataset Indexing Pipeline](#dataset-indexing): the documents that conform the *knowledge base* are preprocessed and transformed into structures that enable fast and easy retrieval of relevant documents. This pipeline is run offline, i.e., as a prior *training* or *preparation* step.
- [Query Pipeline](#query-pipeline): here is where the RAG pattern is run, i.e., the user poses a question, relevant documents are retrieved and inserted into the query prompt and an answer is generated by the LLM.

## Setup

Create an OpenAI account and get an API key; we should save the key in our local and uncommitted `.env`.

Then, create a python environment and install the dependencies:

```bash
# Create the necessary Python environment
conda env create -f conda.yaml
conda activate gennai

# Compile and install all dependencies
pip-compile requirements.in
pip-sync requirements.txt

# If we need a new dependency,
# add it to requirements.in 
# And then:
pip-compile requirements.in
pip-sync requirements.txt
```

Finally, open the notebook where everything is implemented: [`qa_rag.ipynb`](./qa_rag.ipynb).

## Dataset Indexing

The dataset or queried documents are scrapped from the Wikipedia article [2024 Events in Spain](https://en.wikipedia.org/wiki/2024_in_Spain) using `BeautifulSoup`.

The main parsing function is `get_wikipedia_events()`, which returns a list of dictionaries; each dictionary is an event, which contains: 

- `month (str)`: The month of the event.
- `date_text (str)`: The raw date text from the page.
- `date (datetime.date)`: The parsed start date of the event.
- `date_end (Optional[datetime.date])`: The parsed end date if it's a date range, else `None`.
- `event (str)`: The cleaned event description.
- `refs (list[str])`: List of reference IDs from the page.
- `reference_urls (list[str])`: List of URLs for references.
- `reference_entities (list[str])`: List of entities associated with the references.

Then, the the content in the reference URLs is fetched in `get_event_reference_contents()`, which updated the dictionaries with the field `reference_content`.

Additionally, the field `text` is also added in the same function; `text` which is the concatenation of the `date + event + reference_content` fields, i.e., a detailed description of the event.

### Vector Representations

In order to be able to retrieve relevant documents relative to our question/query, we need to 

- preprocess and split our documents properly
- and represent and index our data efficiently.

In our example, as explained, preprocessing and splitting is done at event level: the article is parsed and each event text and metadata (e.g., date, references) are stored in separate documents.

Document/Text representation for fast relevant retrieval is approached in two ways:

- Semantic embeddings: document texts are converted into semantic embeddings, i.e., multi-dimensional dense vectors which point in similar directions if the text have a similar meaning. Embeddings have a fixed dimensionality, defined by the model which generated them. Two embedding models are used:
  - [`intfloat/e5-large-v2`](https://huggingface.co/intfloat/e5-large-v2), from HuggingFace, downloaded and used locally.
  - [`text-embedding-ada-002`](https://platform.openai.com/docs/models/text-embedding-ada-002), from OpenAI, accessed by the API.
- TFIDF indices: *term frequency and inverse document frequency* is technique which sparsely represents the documents by assigning frequency weights to their tokens/terms in relation to their occurrences in the document and in the complete document set. Each token/term in the complete set represents a dimension in the TFIDF vectors.

The combination of several retrieved document sets is also exemplarily shown by implementing *reciprocal rank fusion*.

In addition to the *functional* implementation style followed in the notebook, the class `DataRetriever` is also provided, which compiles the most important functions and transformed data (the indices). `DataRetriever` objects behave and can be used homogeneously independently of the approaches they contain under the hood.

In a regular situation (i.e., not for teaching purposes), it is encouraged to program everything from scratch using the OOP paradigm.

## Query Pipeline

Finally, we implement here `ask_question()`, which integrates all functions and structures defined so far for the answer generation:

- `DataRetriever` objects, which contain:
  - The generated vector indices
  - Transformation and search functions for retrieval
- Retrieval fusion
- Prompt creation
- Call to the response generation via the `completions` API from OpenAI

The same function appends to the generated answer the reference URLs of the selected documents.

## Examples and Comparisons

Finally, different aspects of the implemented system are briefly benchmarked:

1. Which is the difference between using the base model [`gpt-3.5-turbo-instruct`](https://platform.openai.com/docs/models/gpt-3.5-turbo?snapshot=gpt-3.5-turbo-instruct) and the same base model with a context containing query-relevant documents?
2. Which is the difference between the two embedding models used?
3. Which are the differences between the implemented retrievers?
